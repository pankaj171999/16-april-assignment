{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4607f9b-a03b-4d35-809f-eb2889309790",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1(Ans)Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. \n",
    "It is done by building a model by using weak models in series.Firstly, a model is built from the training data. \n",
    "Then the second model is built which tries to correct the errors present in the first model. \n",
    "This procedure is continued and models are added until either the complete training data set is predicted correctly \n",
    "or the maximum number of models are added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fc658-f69f-473a-b6cc-d798004dd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2(Ans)Advantages of Boosting \n",
    "* Improved Accuracy – Boosting can improve the accuracy of the model by combining several weak models’ accuracies \n",
    "and averaging them for regression or voting over them for classification to increase the accuracy of the final model. \n",
    "\n",
    "* Robustness to Overfitting – Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly. \n",
    "\n",
    "* Better handling of imbalanced data – Boosting can handle the imbalance data by focusing more on the data points that are misclassified \n",
    "\n",
    "* Better Interpretability – Boosting can increase the interpretability of the model by breaking \n",
    "the model decision process into multiple processes\n",
    "\n",
    "Disadvantages of Boosting\n",
    "Boosting Algorithms are vulnerable to the outliers.\n",
    "It is difficult to use boosting algorithms for Real-Time applications. It is computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35f429-241d-4882-98bb-b36ce7f5731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3(Ans)In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, \n",
    "each model tries to compensate for the weaknesses of its predecessor. \n",
    "With each iteration, the weak rules from each individual classifier are combined to form one, strong prediction rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4f076-8b27-4593-823b-95240800145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4(Ans)There are three types of Algorithms available such as AdaBoost or Adaptive boosting Algorithm, Gradient, and XG Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469dd64a-8dfa-4faf-87be-270cf64d9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5(Ans) Some common parameters in boosting algorithms are\n",
    "* learning_rate\n",
    "* subsample\n",
    "* n_estimators\n",
    "* Gamma\n",
    "* Max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1db8b1-a5d2-4deb-841a-a063fe7775de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6(Ans)Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. \n",
    "In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate\n",
    "for the weaknesses of its predecessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fadb5c-7ddf-461d-8c5d-42e55df1d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7(Ans)It is a supervised learning algorithm that is used to classify data by combining multiple weak or base learners (e.g., decision trees) into a strong learner\n",
    "\n",
    "Working of Adaboost\n",
    "AdaBoost is implemented by combining several weak learners into a single strong learner. \n",
    "The weak learners in AdaBoost take into account a single input feature and draw out a single split decision tree called the decision stump.\n",
    "Each observation is weighted equally while drawing out the first decision stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3834de-bc76-46bd-ad6a-66f4f1e34a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8(Ans)The error function that AdaBoost uses is an exponential loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85211b58-e73c-41fe-89f5-b9694e9c912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9(Ans)Initially all the data points have equal probability of getting selected, that is each data point has a weight equal to 1/N. \n",
    "In each iteration the weight of a data point gets changed in such a way, that it gets decreased, \n",
    "if it is correctly classified by the model generated in that iteration and increased otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7bf98c-f7d1-4563-935f-8e5147e16ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10(Ans)The learning rate depends highly upon the number of n_estimators. By default, it is set to 1 but it can be increased or decreased depending on the estimators used.\n",
    "Generally, for a large number of n_estimators, we use a smaller value of learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
